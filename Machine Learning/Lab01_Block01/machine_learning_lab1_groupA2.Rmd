---
title: "machine learning(732A99) lab1"
author: "Anubhav Dikshit(anudi287), Lennart Schilling(lensc874), Thijs Quast(thiqu264)"
date: "26 November 2018"
output: 
    pdf_document:
      toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo = FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(xlsx, glmnet, MASS, jtools, huxtable, ggplot2, 
               ggthemes, gridExtra, ROCR, broom, caret, e1071,
               kknn, tidyr, dplyr,reshape2, glmnet)

options("jtools-digits" = 2, scipen = 999)

```


### Contributions

During the lab, Lenart focused on assignment 1, Thijs focused on assignment 3 and Anuhav focused on assignment 4. All codes and analysis was indepedently done and is also reflected in the individual reports.


\newpage
# Assignment 1

## 1.1 Import the data into R and divide it into training and test sets (50%/50%) by using the following code

At first, the data from the Excel file *spambase.xlsx* will be imported and splitted into train and test data (50%:50%)

```{r, echo=TRUE, eval=TRUE}
# Importing data
data = read.xlsx("spambase.xlsx", sheetName = "spambase_data")
tecator_data <- read.xlsx("tecator.xlsx", sheetName = "data")
tecator_data <- tecator_data[,2:NCOL(tecator_data)] # removing sample column

# Dividing data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
```

## 1.2 Use logistic regression (functions glm(), predict()) to classify the training and test data by the classification principles and report the confusion matrices (use table()) and the misclassification rates for training and test data. Analyse the obtained results.

Using the train data, a logistic regression model will be created. Analysing the p-values of the coefficients, it can be seen which independent variables have a significant influence on the dependent variable *Spam*. For the sake of clarity, this is not explicitly stated in this report.

```{r, warning=FALSE}
# Fitting model
logitModel = glm(Spam ~ ., data = train, family = binomial)
summary(logitModel)
```

In the next step, the *logitModel* will be used to classify emails of the training and test data. To prevent duplicate code in 1.3, the *classificationLogit*-function was coded. Giving data and a threshold as an input, a list with the specified threshold to decide which probabilities lead to a spam classification, the confusion matrix and the misclassification rate will be returned. 

```{r, echo=TRUE, eval=TRUE}
# Classifying & evaluating results 
classificationLogit = function(data, threshold = 0.5) {
  # Classifying emails with the model
  yFit = predict(logitModel,
                 newdata = data[,!colnames(data) %in% "Spam"],
                 type='response')
  yFit = ifelse(yFit > threshold, 1, 0)
  # Evaluating classification results
  confusionMatrix = table(y = data$Spam, yFit)
  misclassificationRate <- mean(yFit != data$Spam)
  # Returning results
  return(
    list(
      threshold = threshold,
      confusionMatrix = confusionMatrix,
      misclassificationRate = misclassificationRate
    )
  )
}
```

Using the train data and the default threshold (0.5) as the input leads to the following confusion matrix and misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationLogitTrain = classificationLogit(data = train)
classificationLogitTrain$confusionMatrix
classificationLogitTrain$misclassificationRate
```

Instead, using the test data and the default threshold (0.5) as the input leads to the following confusion matrix and misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationLogitTest = classificationLogit(data = test)
classificationLogitTest$confusionMatrix
classificationLogitTest$misclassificationRate
```

Analysis: It can be seen that the model performs about equally well for both data. The misclassification rate is slightly better for the training data (16.2%) than for the training data (17.7%). This is an indication that there is not too much overfitting on the training data. 

## 1.3 Use logistic regression to classify the test data by the classification principle probability>90%.Assessing the Fit on train dataset for 90% and report the confusion matrices (use table()) and the misclassification rates for training and test data. Compare the results. What effect did the new rule have?

Now we are changing the classification principle and therefore the input threshold from 0.5 to 0.9, however lets also find the best cutoff value

Choosing the best cutoff for test
```{r}

cutoffs <- seq(from = 0.05, to = 0.95, by = 0.05)
accuracy <- NULL
prediction_prob <- predict(logitModel, newdata = test , type = "response")


for (i in seq_along(cutoffs)){
    prediction <- ifelse(prediction_prob >= cutoffs[i], 1, 0) #Predicting for cut-off

    accuracy <- c(accuracy,length(which(test$Spam == prediction))/length(prediction)*100)}

cutoff_data <- as.data.frame(cbind(cutoffs, accuracy))

ggplot(data = cutoff_data, aes(x = cutoffs, y = accuracy)) + 
  geom_line() + 
  ggtitle("Cutoff vs. Accuracy for Test Dataset")

```

Using the train data and the threshold = 0.9 as the input leads to the following confusion matrix and misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationLogitTrainAdjThreshold = classificationLogit(data = train, threshold = 0.9)
classificationLogitTrainAdjThreshold$confusionMatrix
classificationLogitTrainAdjThreshold$misclassificationRate
```

Using the test data and the threshold = 0.9 as the input leads to the following confusion matrix and misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationLogitTestAdjThreshold = classificationLogit(data = test, threshold = 0.9)
classificationLogitTestAdjThreshold$confusionMatrix
classificationLogitTestAdjThreshold$misclassificationRate
```

Analysis: Our small detour suggests that the cutoff value of ~60% was the best for our purpose and going higher than this leads to worse results, at 70% and above the accuracy drastically reduces which is what we see when we make cutoff as 90%.

For both test and train, it can be seen that the classification quality decreases a lot (misclassification rates about 30%). Because the threshold is now much higher than before, the number of false negative predictions has increased strongly.

## 1.4 Use standard classifier kknn() with K=30 from package kknn, report the the misclassification rates for the training and test data and compare the results with step 1.2.

In the following, the standard classifier *kknn()* was used to predict spam mails. Again, to prevent duplicate code in 1.5, the *classificationKknn*-function was coded. Giving data, number of k and a threshold as an input, a list with the same elements as the *classificationLogit*-function is returned.  

```{r, echo=TRUE, eval=TRUE}
library(kknn)
# Classifying & evaluating results 
classificationKknn = function(data, k, threshold = 0.5) {
  # Classifying emails
  kknnModel <- kknn(formula = Spam ~ .,
                    train = train,
                    test = data,
                    k = k)
  kknnModel$fitted.values = ifelse(kknnModel$fitted.values > threshold, 1, 0)
  # Evaluating classification results
  confusionMatrix = table(y = data$Spam, yFit = kknnModel$fitted.values)
  misclassificationRate <- mean(kknnModel$fitted.values != data$Spam)
  # Returning results
  return(
    list(
      threshold = threshold,
      confusionMatrix = confusionMatrix,
      misclassificationRate = misclassificationRate
    )
  )
}
```

Using the train data and k = 30 as the input leads to the following misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationKnnTrain = classificationKknn(data = train, k = 30)
classificationKnnTrain$misclassificationRate
```

Instead, using the test data and k = 30 as the input leads to the following misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationKnnTest = classificationKknn(data = test, k = 30)
classificationKnnTest$misclassificationRate
```

Analysis: Here, a big difference between the prediction power of the model related to the train data (misclassification rate: 17.2%) and the test data (misclassification rate: 32.9%) can be observed. This leads to the assumption that the model is overfitting on the training data. Compared to the results of the logistic regression model with the threshold = 0.5, this model does not deliver such accurate predictions. 

## 1.5 Repeat step 4 for K=1 and compare the results with step 4. What effect does the decrease of K lead to and why?

Now we are changing the k from 30 to 1. 

Using the train data and k = 1 as the input leads to the following misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationKnnTrain = classificationKknn(data = train, k = 1)
classificationKnnTrain$misclassificationRate
```

Using the test data and k = 1 as the input leads to the following misclassification rate.
```{r, echo=TRUE, eval=TRUE}
classificationKnnTest = classificationKknn(data = test, k = 1)
classificationKnnTest$misclassificationRate
```

Analysis: This example shows very clearly that the model is strongly overfitted on the training data. While it classifies every mail for the training data correctly, the misclassification rate for the test data is almost 35%. With k = 1, the classification depends only on the nearest neighbor (the value of the dependent variable of this observation in the training data where the independent variables have the lowest distance to the obsvervation which shall be classified) which leads to a much higher dependency on the training data.

Explaination: The KKNN works in the following way,  An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Thus K=1, makes the seperation boundary to be very complex and locally optimised (lots of local clusters), while as K goes higher, the decision boundary becomes more linear/simple.

# Assignment 3

## 3.1 Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (use only basic R functions)

```{r, warning=FALSE}
select_my_features <- function(x, y, nfolds){
  
  # set seed and reshuffle data
  set.seed(12345)
  intercept <- rep(1, nrow(x))
  matrix_xy <- cbind(intercept, x, y)
  n <- dim(x)[1]
  id <- sample(1:n, floor(n))
  matrix_xy <- matrix_xy[id, ]
  matrix_x <- matrix_xy[, 1:6]
  matrix_y <- matrix_xy[, 7]
  
  # Create folds and empty vectors
  folds <- c(1:nfolds)
  residuals_folds <- c()
  res_model <- c()
  n_features <- c()
  
  # Possible combinations of features including an intercept, intercept is always selected
  combinations_matrix <- expand.grid(c(T, F), c(T, F), c(T, F), c(T, F),
                                     c(T, F))
  intercept_true <- rep(TRUE, 32)
  combinations_matrix <- cbind(intercept_true, combinations_matrix)


    # Loop over each possible model
    for (i in 1:32){
      model_i <- as.logical(combinations_matrix[i,])
      data <- matrix_x[, model_i]
      folds <- c(1:nfolds)
      data_xy <- cbind(data, matrix_y, folds)
      dim_x <- ncol(data_xy) - 2
      
      #loop over each fold
      for (each in 1:nfolds){
        #training and test data
        train <- data_xy[data_xy[, "folds"] != each,]
        train_x <- train[, 1:dim_x]
        y_dim <- dim_x + 1
        train_y <- train[, y_dim]
        
        test <- data_xy[data_xy[, "folds"] == each,]
        test_x <- test[, 1:dim_x]
        test_y <- test[, y_dim]
        
        # computing linear regressions
        Xt_i <- t(train_x)
        XtX_i <- solve(Xt_i %*% train_x)
        betaestimates_i <- XtX_i %*% Xt_i %*% train_y
        yfit_i <- test_x %*% betaestimates_i
        res <- test_y - yfit_i
        mse <- mean(res^2)
        
        #storing outcomes in vectors
        residuals_folds[each] <- mse
        mean_mse <- mean(residuals_folds)
      }
      # storing outcomes in other empty vectors, one level above previous loop
   res_model[i] <- mean_mse
   n_features[i] <- dim_x - 1
    }
  # extracting the best model
  best_model <- which.min(res_model)
  possible_regressors <- colnames(matrix_x)
  x <- as.logical(combinations_matrix[best_model,])
  final_model <- possible_regressors[x]
  
  df <- cbind(res_model, n_features)
  
  # Compute end result
  list_of_results <- list(final_model)
  
  data <- cbind(abs(res), n_features)
  colnames(data) <- c("CV_score", "No_of_features")
  data <- as.data.frame(data)
  
  list_of_results$plot <- ggplot(data = data, aes(x = No_of_features, y = CV_score)) +
    geom_bar(stat="identity") + 
    ggtitle("Barplot of CV Score vs. Features") + coord_flip() 

  
  list_of_results$cv_score <- df[best_model, 1]
  return(list_of_results)
}
```

## 2 Test your function on data set swiss available in the standard R repository:
```{r, warning=FALSE, message=FALSE}
swiss_y <- as.matrix(swiss[, 1])
swiss_x <- as.matrix(swiss[, 2:6])
select_my_features(swiss_x, swiss_y, 5)
```

Analysis: In general I would say that as the number of features increases the model performance increases as well. The optimal subset of featues is 4 (excluding the intercept). The optimal model therefore is:

Fertility ~ Intercept + X1"Agriculture" + X2"Education" + X3"Catholic" + X4"Infant.Mortality"

Resulting in a cross validation score of : 63.40326. 

I would say for none of the independent variables it is reasonable to have an impact on the fertility of people. When reasoning, there is no explanation as to why working in agriculture, having a degree, religion or death of childs could affect the fertility of people. Therefore when computing these models it is always important to reason whether the results make sense.

Infant Mortality is more a result of fertility. Therefore it could be an idea to have "Fertility" as independent variable and "Infant.Mortality" as dependent variable.

# Assignment 4 Linear regression and regularization

## 4.1 Import data to R and create a plot of Moisture versus Protein. Do you think that these data are described well by linear model.
```{r}
ggplot(data = tecator_data, aes(x = Protein, y = Moisture)) + 
  geom_point() + 
  geom_smooth( method = 'loess') +
  ggtitle("Plot of Moisture vs. Protein")
```
Analysis: The data seems fairly linear in nature however there are many outliers. As we can see that data is fairly distributed around the line drawn (above and below) thus there is little bias.

## 4.2 Consider model M in which Moisture is normally distributed, and the expected Moisture is a polynomial function of Protein including the polynomial terms up to power of i. Report a probabilistic model that describes M. Why is it appropriate to use MSE criterion when fitting this model to a training data?

$$ M_i = \sum_{i=0}^{p} X^{i}{Protein} * \beta{i} + \epsilon$$ 

$$\epsilon \sim N\left(0, \sigma^{2} \right)$$

$$\epsilon = M_i - \sum_{i=0}^{p} X^{i}{Protein} * \beta{i}$$

$$ M_i \sim N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right) $$

$$\text{or}$$

$$P \left(M_i | X_{Protein}, \vec{\beta} \right) = N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right)$$ 

$$Where,$$

$$\sigma_{M}^{2}: \text{variance of Moisture}$$

$$p: \text{degree of the polynomial}$$
```{r}
ggplot(data = tecator_data, aes(x = Moisture)) + 
  geom_density() +
  ggtitle("Density Plot of Moisture")
```

Analysis: In this case we are given that mositure is normally distributed, thus the loss function to minimize had to be (actual-predicted)^2, if we were to minimize the absoulte value, then this would assume a Laplace distribution.


## 4.3 Divide the data into training and validation sets (50/50) and fit models M (i=1,2,3,..6). For each model, record the training and the validation MSE and present a plot showing how training and validation MSE depend on i (write some R code to make this plot). Which model is best according to the plot? How do the MSE values change and why? Interpret this picture in terms of bias-variance tradeoff.
```{r}

final_data <- tecator_data

magic_function <- function(df, N)
{
df2 <- df  
for(i in 2:N) 
{
  df2[paste("Protein_",i,"_power", sep="")] <- (df2$Protein)^i
  }

df2 <- df2[c("Protein_2_power", "Protein_3_power", 
             "Protein_4_power", "Protein_5_power", 
             "Protein_6_power")]

df <- cbind(df,df2)  
return(df)
}

final_data <- magic_function(final_data, 6)

set.seed(12345)
n =  NROW(final_data)
id = sample(1:n, floor(n*0.5))
train = final_data[id,]
test = final_data[-id,]

# model building
M_1 <- lm(data = train, Moisture~Protein)
M_2 <- lm(data = train, Moisture~Protein+Protein_2_power)
M_3 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power)
M_4 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power)
M_5 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power)
M_6 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power+Protein_6_power)

train$type <- "train"
test$type <- "test"

final_data <- rbind(test, train)

# predicting new values
M_1_predicted <- predict(M_1, newdata = final_data)
M_2_predicted <- predict(M_2, newdata = final_data)
M_3_predicted <- predict(M_3, newdata = final_data)
M_4_predicted <- predict(M_4, newdata = final_data)
M_5_predicted <- predict(M_5, newdata = final_data)
M_6_predicted <- predict(M_6, newdata = final_data)

# calculating the MSE
final_data$M_1_error <- (final_data$Moisture - M_1_predicted)^2
final_data$M_2_error <- (final_data$Moisture - M_2_predicted)^2
final_data$M_3_error <- (final_data$Moisture - M_3_predicted)^2
final_data$M_4_error <- (final_data$Moisture - M_4_predicted)^2
final_data$M_5_error <- (final_data$Moisture - M_5_predicted)^2
final_data$M_6_error <- (final_data$Moisture - M_6_predicted)^2

# Chainning like Chainsaw
final_error_data <- final_data %>% select(type, M_1_error, M_2_error, M_3_error, 
                                          M_4_error, M_5_error, M_6_error) %>% 
  gather(variable, value, -type) %>% 
  separate(variable, c("model", "power", "error"), "_") %>% 
  group_by(type, power) %>% 
  summarise(MSE = mean(value, na.rm=TRUE))

ggplot(final_error_data, aes(x = power, y = MSE, color=type)) + geom_point() +
  ggtitle("Mean squared error vs. model complexitiy by dataset type")

```
Analysis: As evident from the plot above, we see that as we increase the model complexitiy (higher powers of the 'protein'), the trainning error reduces however the model becomes too biased towards the trainning set (overfits) and misses the test datasets prediction by larger margins in higher powers. 

The best model is M1, that is Moisture~Protein as evident from the least test error (MSE).

The above is a classical case of bias-varience trade-off, which is as follows, as one makes the model fit the trainning dataset better the model becomes more biased and its ability to handle variation to new dataset decreases(varience), thus one should also maintain a good trade off between these two.

## 4.4 Perform variable selection of a linear model in which Fat is response and Channel1:Channel100 are predicted by using stepAIC. Comment on how many variables were selected.
```{r warning=FALSE}
min.model1 = lm(Fat ~ 1, data=tecator_data[,-1])
biggest1 <- formula(lm(Fat ~.,  data=tecator_data[,-1]))

step.model1 <- stepAIC(min.model1, direction ='forward', scope=biggest1, trace = FALSE)
summ(step.model1)
```

Analysis: 29 variables were choose out of 107. Even among these there are many which have very low p values thus statistically it is a practice to remove variables which have p values above 0.05, thus the true variables may not even include these many.

## 4.5 Fit a Ridge regression model with the same predictor and response variables. Present a plot showing how model coefficients depend on the log of the penalty factor lambda and report how the coefficients change with lambda.

```{r}
y <- tecator_data %>% select(Fat) %>% data.matrix()
x <- tecator_data %>% select(-c(Fat)) %>% data.matrix()

lambda <- 10^seq(10, -2, length = 100)

ridge_fit <- glmnet(x, y, alpha = 0, family = "gaussian", lambda = lambda)
plot(ridge_fit, xvar = "lambda", label = TRUE, 
     main = "Plot showing shrinkage of coefficents with rise in log of lambda")


## Change of coefficent with respect to lambda
result <- NULL
for(i in lambda){
temp <- t(coef(ridge_fit, i)) %>% as.matrix()
temp <- cbind(temp, lambda = i)
result <- rbind(temp, result)
}
result <- result %>% as.data.frame() %>% arrange(lambda)
```

```{r echo=TRUE, results='asis'}
table_cofe <- head(result, 10) %>% select(Channel1, Channel2, Channel84, Channel62, 
                                          Channel53, Channel75, Channel57,Protein, 
                                          lambda)

knitr::kable(table_cofe, caption = "Coefficent of Ridge Regression vs. Lambda")
```

Analysis: The idea of lasso and ridge regression is to introduce bias to variables in order to reduce/account for multicollinearity. Introducing bias (lambda) to covarience matrix is done by multiplying the diagonal elements by lambda(often 1+lambda), this inflates the covarience of predictors compared to correleations of predictors. The idea is to test the stability of betas that is how likely are the betas/coefficents of regressions to be stable if we keep introducing bias.

We can clearly see that 'Channel1' and 'Channel2' betas go from positive to negative with very little bias introduced while terms like 'Channel75' dont change the beta signs. Thus the practice is exclude the terms whose beta/coefficent dont change drastically much within say first 10 introduction of lambda.

We see that many of the terms/coefficent tend to zero at around log(lambda) that is ~5.

## 4.6 Repeat step 5 but fit LASSO instead of the Ridge regression and compare the plots from steps 5 and 6. Conclusions? 

```{r}
lambda <- 10^seq(10, -2, length = 100)

lasso_fit <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = lambda)
plot(lasso_fit, xvar = "lambda", label = TRUE, 
     main = "Plot showing shrinkage of coefficents with rise in log of lambda")

```
Analysis: We quickly see that very little introduction of penalisation/bias is all it takes to make many terms/coefficent to zero. This implies for the full dataset lasso is much better suited for regularisation compared to ridge.

At lambda around 1 (log lambda is 0) we get only two or three non zero terms. 

## 4.7 Use cross-validation to find the optimal LASSO model, report the optimal lambda and how many variables were chosen by the model and make conclusions. Present also a plot showing the dependence of the CV score and comment how the CV score changes with lambda.

```{r}
#find the best lambda from our list via cross-validation

lambda_lasso <- 10^seq(10, -2, length = 100)
lasso_cv <- cv.glmnet(x,y, alpha=1, lambda = lambda_lasso, type.measure="mse")
coef(lasso_cv, lambda = lasso_cv$lambda.min)

lasso_cv$lambda.min
  
## Change of coefficent with respect to lambda
result_lasso <- NULL
for(i in 1:length(lambda_lasso)){
temp <- lasso_cv$cvm[i] %>% as.matrix()
temp <- cbind(CVM_error = temp, lambda = lasso_cv$lambda[i])
result_lasso <- rbind(temp, result_lasso)
}

```

```{r echo=TRUE, results='asis'}
result_lasso <- result_lasso %>% as.data.frame() %>% arrange(lambda)
colnames(result_lasso) <- c("Cross_Mean_Error", "Lambda")

ggplot(result_lasso, aes(x = log(Lambda), y = Cross_Mean_Error)) + geom_point() + 
  ggtitle("Cross Validation Error vs. Lambda")
```

Analysis: The minimum value of lambda was 0.1, implies almost zero penalisation. The variables selected are: 
Channel98, Channel99, Channel100, Protein, Moisture, Channel37, Channel38, Channel39, Channel40 along with intercept.

We see that Cross validation error is lowest at lambda 0.1 and remains low till lambda~1 (log lambda 0) after which the error drastically increases at log(lambda) ~ 2.5, the error maxes out and remains about the same for higher values of lambda. This implies that more bias introduction will lead to worse performance.

## 4.8 Compare the results from steps 4 and 7.

Analysis: In order to find the best predictors for a given model we employ various techniques.

In step4 we analytically arrive at the best variables to model 'Fat' using multiple variables, using stepAIC we arrive at 29 variables excluding the Intercept.

In step5 we use regularisation techniques and start introducing bias to eliminate the variables which maybe corellated with each other, employing this we get further reduction of about 10 variables at log lambda ~ 5.

Using Lasso in step6 we get only about 5 variables at lambda close to 1, however the exact number of variables to choose is depended on the lambda value and the corresponding error. However having used the whole dataset, we need to employee cross validation to get the precise value of lambda.

In step 7 we get the best value of lambda for lasso for which the mean squared error is the least and here we are left with 9 variables excluding the intercept. The mean squared error is also low (~10).

Thus we have learned how to perform regression and how to account for multicorrlinearity and possible ways to detect and negate the same.

# Apendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```